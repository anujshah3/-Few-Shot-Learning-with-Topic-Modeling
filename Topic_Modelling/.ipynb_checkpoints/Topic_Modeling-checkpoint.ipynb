{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic \n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora;\n",
    "import pickle\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string: see comments in the source code for more info\n",
    "    \"\"\"\n",
    "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
    "    # normalization 2: lower case\n",
    "    s = s.lower()\n",
    "    # normalization 3: \"&gt\", \"&lt\"\n",
    "    s = re.sub(r'&gt|&lt', ' ', s)\n",
    "    # normalization 4: letter repetition (if more than 2)\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # normalization 5: non-word repetition (if more than 1)\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # normalization 6: string * as delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
    "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
    "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
    "    # normalization 8: xxx[?!]. -- > xxx.\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "    # normalization 9: [.?!] --> [.?!] xxx\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "    # normalization 10: ' ing ', noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # normalization 11: noise text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "    # normalization 12: phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "    #removing \\n\n",
    "    s = re.sub(r'\\n',r' ', s)\n",
    "    #removing §\n",
    "    s = re.sub(r'§',r' ', s)\n",
    "    #remove numbers\n",
    "    s = re.sub(r'([0-9]q|[0-9]|-\\*[0-9]|(\\w)[0-9])',r'',s)\n",
    "    #remove -'s\n",
    "    s = re.sub(r'(-|–\\*|. –)',r'',s)\n",
    "     #remove -'s\n",
    "    #s = re.sub(r'-',r'',s)\n",
    "    \n",
    "    #remove roman numbers\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    s = re.sub(pattern, '', s)\n",
    "    \n",
    "\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_data(file):\n",
    "    data = []\n",
    "    pdfFileObj = open(file, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj,strict=False)\n",
    "    number_of_pages = len(pdfReader.pages)\n",
    "    for line in range(0,number_of_pages):\n",
    "        #line_data = pdfReader.getPage(line).extractText().replace(\" -\\n\",\"\")\n",
    "        line_data = f_base(pdfReader.getPage(line).extractText().replace(\" -\\n\",\"\"))\n",
    "        data.append(line_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for year: 18-1\n",
      "Reading data for year: 18-2\n",
      "Reading data for year: 18-3\n",
      "Reading data for year: 18-4\n",
      "Reading data for year: 18-5\n",
      "Reading data for year: 18-6\n",
      "Reading data for year: 18-7\n",
      "Reading data for year: 18-8\n",
      "Reading data for year: 18-9\n",
      "Reading data for year: 18-10\n",
      "Reading data for year: 18-11\n",
      "Reading data for year: 18-12\n",
      "Reading data for year: 18-13\n",
      "Reading data for year: 18-14\n",
      "Reading data for year: 18-15\n",
      "Reading data for year: 18-16\n",
      "Reading data for year: 18-17\n",
      "Reading data for year: 18-18\n",
      "Reading data for year: 18-19\n",
      "Reading data for year: 18-20\n",
      "Reading data for year: 18-21\n",
      "Reading data for year: 18-22\n",
      "Reading data for year: 18-23\n",
      "Reading data for year: 18-24\n",
      "Reading data for year: 18-25\n",
      "Reading data for year: 18-26\n",
      "Reading data for year: 18-27\n",
      "Reading data for year: 18-28\n",
      "Reading data for year: 18-29\n",
      "Reading data for year: 18-30\n",
      "Reading data for year: 18-31\n",
      "Reading data for year: 18-32\n",
      "Reading data for year: 18-33\n",
      "Reading data for year: 18-34\n",
      "Reading data for year: 18-35\n",
      "Reading data for year: 18-36\n",
      "Reading data for year: 18-37\n",
      "Reading data for year: 18-38\n",
      "Reading data for year: 18-39\n",
      "Reading data for year: 18-40\n",
      "Reading data for year: 18-41\n",
      "Reading data for year: 18-42\n",
      "Reading data for year: 18-43\n",
      "Reading data for year: 18-44\n",
      "Reading data for year: 18-45\n",
      "Reading data for year: 18-46\n",
      "Reading data for year: 18-47\n",
      "Reading data for year: 18-48\n",
      "Reading data for year: 19-1\n",
      "Reading data for year: 19-2\n",
      "Reading data for year: 19-3\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "for year in range(18, 23):\n",
    "    for week in range(1,49):\n",
    "        print(f\"Reading data for year: {year}-{week}\")\n",
    "        if week > 9: \n",
    "            data_dict[str(year)+\"-\"+str(week)] = read_pdf_data(f\"..//data//irb{year}-{week}.pdf\")\n",
    "        else:\n",
    "            data_dict[str(year)+\"-\"+\"0\"+str(week)] = read_pdf_data(f\"..//data//irb{year}-0{week}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list = [data_dict[str(year)+\"-\"+str(week)] for year in range(18,23) for week in range(1, 49)]\n",
    "data_list = []\n",
    "for year in range(18, 23):\n",
    "    for week in range(1,49):\n",
    "        #print(f\"Adding data for year: {year}-{week}\")\n",
    "        if week > 9: \n",
    "            data_list.append(data_dict[str(year)+\"-\"+str(week)])\n",
    "            print(f\"Adding data for year: {year}-{week}\")\n",
    "        else:\n",
    "            data_list.append(data_dict[str(year)+\"-\"+\"0\"+str(week)])\n",
    "            print(f\"Adding data for year: {year}-0{week}\")\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "for sublist in data_list:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "        \n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list =[]\n",
    "key_dict = {\"18\":\"2018\",\"19\":\"2019\",\"20\":\"2020\",\"21\":\"2021\",\"22\":\"2022\"}\n",
    "for key in data_dict.keys():\n",
    "    for val in data_dict[key]:\n",
    "        key_list.append(key_dict[key.split('-')[0]])\n",
    "len(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keybert import KeyBERT\n",
    "# # Extract keywords\n",
    "# kw_model = KeyBERT()\n",
    "# keywords = kw_model.extract_keywords(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = []\n",
    "\n",
    "# Folder Path\n",
    "path = \"stopwords\"\n",
    "  \n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for word in f.readlines():\n",
    "            custom_stop_words.append(word.lower().replace('\\n',''))\n",
    "\n",
    "for (root, dirs, file) in os.walk(path): \n",
    "    for f in file:\n",
    "        if '.txt' in f:\n",
    "            print(f)\n",
    "            read_text_file(path+\"/\"+f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(custom_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model_basic = BERTopic(verbose=True, embedding_model=sentence_model, min_topic_size= 10)\n",
    "topics_basic, probs_basic = model_basic.fit_transform(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_basic = model_basic.get_topic_info()\n",
    "print(\"Number of topics: {}\".format( len(freq_basic)))\n",
    "freq_basic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basic.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic = pd.DataFrame({\"topic\":topics_basic,\"docs\": flat_list})\n",
    "df_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basic.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basic.visualize_barchart(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = flat_list\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=25)\n",
    "embeddings = vectorizer.fit_transform(docs)\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=my_stop_words)\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model, n_gram_range=(2, 3))\n",
    "#topic_model = BERTopic(n_gram_range=(2, 3))\n",
    "\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "print(\"Number of topics: {}\".format( len(freq)))\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=10, metric='cosine', low_memory=False)\n",
    "topic_model_umap = BERTopic(umap_model=umap_model,vectorizer_model=vectorizer_model, n_gram_range=(2, 3)).fit(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_umap.visualize_barchart(top_n_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_umap.visualize_documents(docs, embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = UMAP(n_neighbors=40, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model_umap.visualize_documents(docs, reduced_embeddings=reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c-TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "topic_model_ctf_idf = BERTopic(ctfidf_model=ctfidf_model,vectorizer_model=vectorizer_model).fit(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_m2 = topic_model_ctf_idf.get_topic_info()\n",
    "print(\"Number of topics: {}\".format( len(freq_m2)))\n",
    "freq_m2.head()\n",
    "#.visualize_barchart(top_n_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_ctf_idf.visualize_barchart(top_n_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model_ctf_idf.topics_over_time(flat_list, key_list, nr_bins=20)\n",
    "\n",
    "topic_model_ctf_idf.visualize_topics_over_time(topics_over_time, topics=[4, 5, 7, 12, 13, 24, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model_hdb = BERTopic(hdbscan_model=hdbscan_model,vectorizer_model=vectorizer_model).fit(flat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_hdb.visualize_barchart(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Perplexity With LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data = flat_list[:5000]\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(custom_stop_words)\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tm",
   "language": "python",
   "name": "venv_tm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa3053b19b748457bbedb3545673427c7c005ac253b354eaff3f2402df8b4632"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
